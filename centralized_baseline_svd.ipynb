{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Centralized BPR-MF Baseline with Bayesian Optimization\n",
    "\n",
    "This notebook implements a **Centralized BPR-MF baseline** for comparison with federated learning approaches.\n",
    "\n",
    "## Key Features\n",
    "- **Bayesian Optimization** using Optuna for hyperparameter tuning\n",
    "- **BPR-MF** - Bayesian Personalized Ranking Matrix Factorization\n",
    "- **Sampled Evaluation** - 1 positive + 99 negatives (NCF paper protocol)\n",
    "- **Metrics**: HR@K, NDCG@K, MRR (same as federated experiments)\n",
    "\n",
    "## Hyperparameters Optimized\n",
    "| Parameter | Search Space | Type |\n",
    "|-----------|-------------|------|\n",
    "| embedding_dim | [32, 64, 128, 256] | Categorical |\n",
    "| learning_rate | [1e-4, 0.1] | Log-uniform |\n",
    "| weight_decay | [1e-7, 1e-4] | Log-uniform |\n",
    "| num_negatives | [1, 2, 4, 8] | Categorical |\n",
    "| batch_size | [256, 512, 1024, 2048] | Categorical |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_section",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251122+cu128\n",
      "Optuna version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "# Bayesian Optimization\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner, SuccessiveHalvingPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "seed_device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "  - Optimization trials: 50\n",
      "  - Objective metric: ndcg@10\n",
      "  - Epochs per trial: 50\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for BPR-MF experiments with Bayesian Optimization.\"\"\"\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET = 'ml-1m'\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Directories\n",
    "    RESULTS_DIR = 'results/centralized'\n",
    "    FIGURES_DIR = 'figures'\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Bayesian Optimization Settings\n",
    "    # ==========================================================================\n",
    "    OPTUNA_N_TRIALS = 50           # Number of optimization trials\n",
    "    OPTUNA_TIMEOUT = None          # Timeout in seconds (None = no timeout)\n",
    "    OPTUNA_N_STARTUP_TRIALS = 10   # Random trials before TPE kicks in\n",
    "    OPTUNA_PRUNING_WARMUP = 10     # Epochs before pruning can occur\n",
    "    OPTUNA_PRUNING_INTERVAL = 5    # Evaluate every N epochs for pruning\n",
    "    \n",
    "    # Optimization objective\n",
    "    OPTUNA_METRIC = 'ndcg@10'      # Metric to optimize (ndcg@10 or hr@10)\n",
    "    OPTUNA_DIRECTION = 'maximize'  # maximize or minimize\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Hyperparameter Search Space\n",
    "    # ==========================================================================\n",
    "    SEARCH_SPACE = {\n",
    "        'embedding_dim': [32, 64, 128, 256],\n",
    "        'lr_min': 1e-4,\n",
    "        'lr_max': 0.1,\n",
    "        'weight_decay_min': 1e-7,\n",
    "        'weight_decay_max': 1e-4,\n",
    "        'num_negatives': [1, 2, 4, 8],\n",
    "        'batch_size': [256, 512, 1024, 2048],\n",
    "    }\n",
    "    \n",
    "    # Fixed training parameters\n",
    "    N_EPOCHS_TUNING = 50           # Epochs during hyperparameter search\n",
    "    N_EPOCHS_FINAL = 100           # Epochs for final training with best params\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Evaluation Settings (matching federated setup)\n",
    "    # ==========================================================================\n",
    "    EVAL_NUM_NEGATIVES = 99        # 1 positive + 99 negatives (NCF paper protocol)\n",
    "    RANKING_K_VALUES = [5, 10, 20]\n",
    "    EVAL_SAMPLE_USERS = 1000       # Sample users for fast evaluation during tuning\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Default BPR-MF Parameters (fallback if not tuning)\n",
    "    # ==========================================================================\n",
    "    DEFAULT_PARAMS = {\n",
    "        'embedding_dim': 128,\n",
    "        'lr': 0.01,\n",
    "        'weight_decay': 1e-6,\n",
    "        'n_epochs': 100,\n",
    "        'batch_size': 1024,\n",
    "        'num_negatives': 4,\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "        os.makedirs(self.FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  - Optimization trials: {config.OPTUNA_N_TRIALS}\")\n",
    "print(f\"  - Objective metric: {config.OPTUNA_METRIC}\")\n",
    "print(f\"  - Epochs per trial: {config.N_EPOCHS_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_section",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movielens_1m():\n",
    "    \"\"\"\n",
    "    Load MovieLens-1M dataset as pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame with user_idx, item_idx, rating, timestamp\n",
    "        num_users: Number of unique users\n",
    "        num_items: Number of unique items\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING MOVIELENS-1M DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load from surprise's cached location (handles download automatically)\n",
    "    from surprise import Dataset\n",
    "    data = Dataset.load_builtin('ml-1m')\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    raw_ratings = data.raw_ratings\n",
    "    df = pd.DataFrame(raw_ratings, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Convert types\n",
    "    df['rating'] = df['rating'].astype(float)\n",
    "    df['timestamp'] = df['timestamp'].astype(int)\n",
    "    \n",
    "    # Create contiguous ID mappings (required for embedding layers)\n",
    "    unique_users = df['user_id'].unique()\n",
    "    unique_items = df['item_id'].unique()\n",
    "    \n",
    "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "    \n",
    "    df['user_idx'] = df['user_id'].map(user_to_idx)\n",
    "    df['item_idx'] = df['item_id'].map(item_to_idx)\n",
    "    \n",
    "    num_users = len(unique_users)\n",
    "    num_items = len(unique_items)\n",
    "    num_ratings = len(df)\n",
    "    \n",
    "    print(f\"\\u2713 Dataset loaded successfully\")\n",
    "    print(f\"  - Users: {num_users:,}\")\n",
    "    print(f\"  - Items: {num_items:,}\")\n",
    "    print(f\"  - Ratings: {num_ratings:,}\")\n",
    "    print(f\"  - Sparsity: {(1 - num_ratings/(num_users*num_items))*100:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    return df, num_users, num_items\n",
    "\n",
    "\n",
    "def train_test_split_leave_one_out(df):\n",
    "    \"\"\"\n",
    "    Leave-one-out split: last interaction per user for test.\n",
    "    \n",
    "    Returns:\n",
    "        train_df: Training interactions\n",
    "        test_df: Test interactions (one per user)\n",
    "        train_user_items: Dict mapping user_idx -> set of positive item_idx\n",
    "    \"\"\"\n",
    "    print(\"Splitting data (leave-one-out)...\")\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(['user_idx', 'timestamp'])\n",
    "    \n",
    "    # Get last interaction per user for test\n",
    "    test_df = df.groupby('user_idx').last().reset_index()\n",
    "    \n",
    "    # Remove test interactions from training\n",
    "    test_pairs = set(zip(test_df['user_idx'], test_df['item_idx']))\n",
    "    train_mask = ~df.apply(lambda x: (x['user_idx'], x['item_idx']) in test_pairs, axis=1)\n",
    "    train_df = df[train_mask].copy()\n",
    "    \n",
    "    # Build user -> positive items mapping\n",
    "    train_user_items = defaultdict(set)\n",
    "    for _, row in train_df.iterrows():\n",
    "        train_user_items[row['user_idx']].add(row['item_idx'])\n",
    "    \n",
    "    print(f\"\\u2713 Train size: {len(train_df):,} ratings\")\n",
    "    print(f\"\\u2713 Test size:  {len(test_df):,} ratings\")\n",
    "    print()\n",
    "    \n",
    "    return train_df, test_df, train_user_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "# BPR-MF Model\n",
    "\n",
    "Bayesian Personalized Ranking with Matrix Factorization.\n",
    "\n",
    "**Architecture:**\n",
    "- User embeddings: $\\mathbf{p}_u \\in \\mathbb{R}^d$\n",
    "- Item embeddings: $\\mathbf{q}_i \\in \\mathbb{R}^d$\n",
    "- Score: $\\hat{r}_{ui} = \\mathbf{p}_u^T \\mathbf{q}_i + b_u + b_i + \\mu$\n",
    "\n",
    "**BPR Loss:**\n",
    "$$\\mathcal{L} = -\\sum_{(u,i,j)} \\log \\sigma(\\hat{r}_{ui} - \\hat{r}_{uj})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bpr_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking Matrix Factorization.\n",
    "    \n",
    "    This is the centralized version matching the federated implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, use_bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Biases\n",
    "        if use_bias:\n",
    "            self.user_bias = nn.Embedding(num_users, 1)\n",
    "            self.item_bias = nn.Embedding(num_items, 1)\n",
    "            self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier uniform.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            nn.init.normal_(self.user_bias.weight, mean=0.0, std=0.01)\n",
    "            nn.init.normal_(self.item_bias.weight, mean=0.0, std=0.01)\n",
    "            nn.init.zeros_(self.global_bias)\n",
    "    \n",
    "    def forward(self, user_ids, pos_item_ids, neg_item_ids=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            user_ids: (batch_size,)\n",
    "            pos_item_ids: (batch_size,)\n",
    "            neg_item_ids: (batch_size,) or None for inference\n",
    "            \n",
    "        Returns:\n",
    "            If neg_item_ids provided: (pos_scores, neg_scores) for BPR loss\n",
    "            Otherwise: pos_scores for inference\n",
    "        \"\"\"\n",
    "        user_emb = self.user_embeddings(user_ids)\n",
    "        pos_item_emb = self.item_embeddings(pos_item_ids)\n",
    "        \n",
    "        # Compute positive scores\n",
    "        pos_scores = torch.sum(user_emb * pos_item_emb, dim=1)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            user_b = self.user_bias(user_ids).squeeze(-1)\n",
    "            pos_item_b = self.item_bias(pos_item_ids).squeeze(-1)\n",
    "            pos_scores = self.global_bias + user_b + pos_item_b + pos_scores\n",
    "        \n",
    "        if neg_item_ids is not None:\n",
    "            neg_item_emb = self.item_embeddings(neg_item_ids)\n",
    "            neg_scores = torch.sum(user_emb * neg_item_emb, dim=1)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                neg_item_b = self.item_bias(neg_item_ids).squeeze(-1)\n",
    "                neg_scores = self.global_bias + user_b + neg_item_b + neg_scores\n",
    "            \n",
    "            return pos_scores, neg_scores\n",
    "        \n",
    "        return pos_scores\n",
    "    \n",
    "    def predict(self, user_ids, item_ids):\n",
    "        \"\"\"Predict scores for user-item pairs.\"\"\"\n",
    "        user_emb = self.user_embeddings(user_ids)\n",
    "        item_emb = self.item_embeddings(item_ids)\n",
    "        \n",
    "        scores = torch.sum(user_emb * item_emb, dim=1)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            user_b = self.user_bias(user_ids).squeeze(-1)\n",
    "            item_b = self.item_bias(item_ids).squeeze(-1)\n",
    "            scores = self.global_bias + user_b + item_b + scores\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_section",
   "metadata": {},
   "source": [
    "# BPR Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bpr_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    Dataset for BPR training with negative sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, num_items, num_negatives=1):\n",
    "        self.num_items = num_items\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        # Store positive interactions\n",
    "        self.users = df['user_idx'].values\n",
    "        self.pos_items = df['item_idx'].values\n",
    "        \n",
    "        # Build user -> positive items mapping for negative sampling\n",
    "        self.user_positive_items = defaultdict(set)\n",
    "        for user, item in zip(self.users, self.pos_items):\n",
    "            self.user_positive_items[user].add(item)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users) * self.num_negatives\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = idx // self.num_negatives\n",
    "        user = self.users[pos_idx]\n",
    "        pos_item = self.pos_items[pos_idx]\n",
    "        \n",
    "        # Sample negative item\n",
    "        neg_item = np.random.randint(self.num_items)\n",
    "        while neg_item in self.user_positive_items[user]:\n",
    "            neg_item = np.random.randint(self.num_items)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user, dtype=torch.long),\n",
    "            torch.tensor(pos_item, dtype=torch.long),\n",
    "            torch.tensor(neg_item, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "training_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    \"\"\"Bayesian Personalized Ranking loss.\"\"\"\n",
    "    return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train BPR-MF for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "        user_ids = user_ids.to(device)\n",
    "        pos_item_ids = pos_item_ids.to(device)\n",
    "        neg_item_ids = neg_item_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pos_scores, neg_scores = model(user_ids, pos_item_ids, neg_item_ids)\n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def train_epoch_verbose(model, train_loader, optimizer, device, epoch):\n",
    "    \"\"\"Train BPR-MF for one epoch with progress bar.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for user_ids, pos_item_ids, neg_item_ids in pbar:\n",
    "        user_ids = user_ids.to(device)\n",
    "        pos_item_ids = pos_item_ids.to(device)\n",
    "        neg_item_ids = neg_item_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pos_scores, neg_scores = model(user_ids, pos_item_ids, neg_item_ids)\n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_section",
   "metadata": {},
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "Sampled evaluation following the NCF paper protocol:\n",
    "- For each test interaction (user, positive_item)\n",
    "- Sample 99 negative items that the user hasn't interacted with\n",
    "- Rank all 100 items (1 positive + 99 negatives)\n",
    "- Compute HR@K, NDCG@K, MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eval_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_metrics(\n",
    "    model, \n",
    "    test_df, \n",
    "    train_user_items, \n",
    "    num_items,\n",
    "    num_negatives=99, \n",
    "    k_values=[5, 10, 20], \n",
    "    device='cpu',\n",
    "    sample_users=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute ranking metrics using sampled evaluation protocol.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BPR-MF model\n",
    "        test_df: Test DataFrame with user_idx, item_idx\n",
    "        train_user_items: Dict mapping user_idx -> set of positive item_idx\n",
    "        num_items: Total number of items\n",
    "        num_negatives: Number of negative samples (default: 99)\n",
    "        k_values: List of K values for metrics\n",
    "        device: Device to use\n",
    "        sample_users: If set, sample this many users for faster evaluation\n",
    "        verbose: Whether to show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    hits = {k: [] for k in k_values}\n",
    "    ndcgs = {k: [] for k in k_values}\n",
    "    mrrs = []\n",
    "    \n",
    "    # Get unique test users and their positive items\n",
    "    test_user_items = defaultdict(list)\n",
    "    for _, row in test_df.iterrows():\n",
    "        test_user_items[row['user_idx']].append(row['item_idx'])\n",
    "    \n",
    "    # Optionally sample users for faster evaluation\n",
    "    user_list = list(test_user_items.keys())\n",
    "    if sample_users is not None and sample_users < len(user_list):\n",
    "        user_list = np.random.choice(user_list, sample_users, replace=False)\n",
    "    \n",
    "    iterator = tqdm(user_list, desc=\"Evaluating\") if verbose else user_list\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_idx in iterator:\n",
    "            pos_items = test_user_items[user_idx]\n",
    "            train_positives = train_user_items.get(user_idx, set())\n",
    "            \n",
    "            for pos_item in pos_items:\n",
    "                # Sample negative items\n",
    "                negatives = []\n",
    "                all_positives = train_positives | {pos_item}\n",
    "                \n",
    "                while len(negatives) < num_negatives:\n",
    "                    neg = np.random.randint(num_items)\n",
    "                    if neg not in all_positives and neg not in negatives:\n",
    "                        negatives.append(neg)\n",
    "                \n",
    "                # Create candidate list: positive + negatives\n",
    "                candidates = [pos_item] + negatives\n",
    "                \n",
    "                # Get scores\n",
    "                user_tensor = torch.tensor([user_idx] * len(candidates), dtype=torch.long, device=device)\n",
    "                item_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "                \n",
    "                scores = model.predict(user_tensor, item_tensor)\n",
    "                scores = scores.cpu().numpy()\n",
    "                \n",
    "                # Get ranking (descending order)\n",
    "                ranking = np.argsort(-scores)\n",
    "                \n",
    "                # Position of positive item (0-indexed)\n",
    "                pos_rank = np.where(ranking == 0)[0][0]\n",
    "                \n",
    "                # Compute metrics\n",
    "                for k in k_values:\n",
    "                    hits[k].append(1.0 if pos_rank < k else 0.0)\n",
    "                    \n",
    "                    if pos_rank < k:\n",
    "                        ndcgs[k].append(1.0 / np.log2(pos_rank + 2))\n",
    "                    else:\n",
    "                        ndcgs[k].append(0.0)\n",
    "                \n",
    "                # MRR\n",
    "                mrrs.append(1.0 / (pos_rank + 1))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        results[f'hr@{k}'] = np.mean(hits[k])\n",
    "        results[f'ndcg@{k}'] = np.mean(ndcgs[k])\n",
    "    results['mrr'] = np.mean(mrrs)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayesian_section",
   "metadata": {},
   "source": [
    "# Bayesian Optimization with Optuna\n",
    "\n",
    "We use **Tree-structured Parzen Estimator (TPE)** sampler with **Median Pruning** for early stopping of unpromising trials.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic hyperparameter search\n",
    "- Early stopping of bad trials (pruning)\n",
    "- Visualization of optimization history\n",
    "- Parameter importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "optuna_objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaObjective:\n",
    "    \"\"\"\n",
    "    Optuna objective function for BPR-MF hyperparameter optimization.\n",
    "    \n",
    "    Uses pruning to early-stop unpromising trials.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_df, test_df, train_user_items, num_users, num_items, device):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.train_user_items = train_user_items\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.device = device\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        # =================================================================\n",
    "        # Sample hyperparameters\n",
    "        # =================================================================\n",
    "        embedding_dim = trial.suggest_categorical(\n",
    "            'embedding_dim', \n",
    "            config.SEARCH_SPACE['embedding_dim']\n",
    "        )\n",
    "        lr = trial.suggest_float(\n",
    "            'lr', \n",
    "            config.SEARCH_SPACE['lr_min'], \n",
    "            config.SEARCH_SPACE['lr_max'], \n",
    "            log=True\n",
    "        )\n",
    "        weight_decay = trial.suggest_float(\n",
    "            'weight_decay', \n",
    "            config.SEARCH_SPACE['weight_decay_min'], \n",
    "            config.SEARCH_SPACE['weight_decay_max'], \n",
    "            log=True\n",
    "        )\n",
    "        num_negatives = trial.suggest_categorical(\n",
    "            'num_negatives', \n",
    "            config.SEARCH_SPACE['num_negatives']\n",
    "        )\n",
    "        batch_size = trial.suggest_categorical(\n",
    "            'batch_size', \n",
    "            config.SEARCH_SPACE['batch_size']\n",
    "        )\n",
    "        \n",
    "        # =================================================================\n",
    "        # Create dataset and model\n",
    "        # =================================================================\n",
    "        train_dataset = BPRDataset(\n",
    "            self.train_df, \n",
    "            self.num_items, \n",
    "            num_negatives=num_negatives\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        model = BPRMF(\n",
    "            num_users=self.num_users,\n",
    "            num_items=self.num_items,\n",
    "            embedding_dim=embedding_dim,\n",
    "            use_bias=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # =================================================================\n",
    "        # Training loop with pruning\n",
    "        # =================================================================\n",
    "        best_metric = 0.0\n",
    "        \n",
    "        for epoch in range(1, config.N_EPOCHS_TUNING + 1):\n",
    "            # Train one epoch\n",
    "            train_epoch(model, train_loader, optimizer, self.device)\n",
    "            \n",
    "            # Evaluate periodically for pruning\n",
    "            if epoch >= config.OPTUNA_PRUNING_WARMUP and epoch % config.OPTUNA_PRUNING_INTERVAL == 0:\n",
    "                metrics = compute_ranking_metrics(\n",
    "                    model=model,\n",
    "                    test_df=self.test_df,\n",
    "                    train_user_items=self.train_user_items,\n",
    "                    num_items=self.num_items,\n",
    "                    num_negatives=config.EVAL_NUM_NEGATIVES,\n",
    "                    k_values=[10],  # Only compute @10 for speed\n",
    "                    device=self.device,\n",
    "                    sample_users=config.EVAL_SAMPLE_USERS,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                current_metric = metrics[config.OPTUNA_METRIC]\n",
    "                best_metric = max(best_metric, current_metric)\n",
    "                \n",
    "                # Report to Optuna for pruning\n",
    "                trial.report(current_metric, epoch)\n",
    "                \n",
    "                # Check if trial should be pruned\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "        \n",
    "        # =================================================================\n",
    "        # Final evaluation\n",
    "        # =================================================================\n",
    "        final_metrics = compute_ranking_metrics(\n",
    "            model=model,\n",
    "            test_df=self.test_df,\n",
    "            train_user_items=self.train_user_items,\n",
    "            num_items=self.num_items,\n",
    "            num_negatives=config.EVAL_NUM_NEGATIVES,\n",
    "            k_values=[10],\n",
    "            device=self.device,\n",
    "            sample_users=config.EVAL_SAMPLE_USERS,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return final_metrics[config.OPTUNA_METRIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "run_optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bayesian_optimization(train_df, test_df, train_user_items, num_users, num_items):\n",
    "    \"\"\"\n",
    "    Run Bayesian Optimization to find best hyperparameters.\n",
    "    \n",
    "    Returns:\n",
    "        study: Optuna study object with optimization results\n",
    "        best_params: Dictionary of best hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BAYESIAN OPTIMIZATION (Optuna)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSearch Space:\")\n",
    "    print(f\"  - embedding_dim: {config.SEARCH_SPACE['embedding_dim']}\")\n",
    "    print(f\"  - lr: [{config.SEARCH_SPACE['lr_min']}, {config.SEARCH_SPACE['lr_max']}] (log)\")\n",
    "    print(f\"  - weight_decay: [{config.SEARCH_SPACE['weight_decay_min']}, {config.SEARCH_SPACE['weight_decay_max']}] (log)\")\n",
    "    print(f\"  - num_negatives: {config.SEARCH_SPACE['num_negatives']}\")\n",
    "    print(f\"  - batch_size: {config.SEARCH_SPACE['batch_size']}\")\n",
    "    print(f\"\\nOptimization Settings:\")\n",
    "    print(f\"  - Number of trials: {config.OPTUNA_N_TRIALS}\")\n",
    "    print(f\"  - Epochs per trial: {config.N_EPOCHS_TUNING}\")\n",
    "    print(f\"  - Objective metric: {config.OPTUNA_METRIC}\")\n",
    "    print(f\"  - Pruning warmup: {config.OPTUNA_PRUNING_WARMUP} epochs\")\n",
    "    print(f\"  - Eval sample users: {config.EVAL_SAMPLE_USERS}\")\n",
    "    print()\n",
    "    \n",
    "    # Create objective function\n",
    "    objective = OptunaObjective(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        train_user_items=train_user_items,\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create study with TPE sampler and median pruner\n",
    "    sampler = TPESampler(\n",
    "        seed=config.RANDOM_STATE,\n",
    "        n_startup_trials=config.OPTUNA_N_STARTUP_TRIALS\n",
    "    )\n",
    "    \n",
    "    pruner = MedianPruner(\n",
    "        n_startup_trials=config.OPTUNA_N_STARTUP_TRIALS,\n",
    "        n_warmup_steps=config.OPTUNA_PRUNING_WARMUP,\n",
    "        interval_steps=config.OPTUNA_PRUNING_INTERVAL\n",
    "    )\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=config.OPTUNA_DIRECTION,\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name='bpr_mf_optimization'\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Starting optimization...\\n\")\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=config.OPTUNA_N_TRIALS,\n",
    "        timeout=config.OPTUNA_TIMEOUT,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest trial:\")\n",
    "    print(f\"  Value ({config.OPTUNA_METRIC}): {study.best_trial.value:.4f}\")\n",
    "    print(f\"  Params:\")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"    - {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    \n",
    "    # Statistics\n",
    "    pruned_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])\n",
    "    complete_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    print(f\"\\nTrial Statistics:\")\n",
    "    print(f\"  - Completed: {complete_trials}\")\n",
    "    print(f\"  - Pruned: {pruned_trials}\")\n",
    "    print(f\"  - Total: {len(study.trials)}\")\n",
    "    print()\n",
    "    \n",
    "    return study, study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_section",
   "metadata": {},
   "source": [
    "# Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimization_results(study, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot Optuna optimization results.\n",
    "    \"\"\"\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Optimization History\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.update_layout(title='Optimization History')\n",
    "    if save_dir:\n",
    "        fig.write_image(os.path.join(save_dir, 'optimization_history.png'))\n",
    "    fig.show()\n",
    "    \n",
    "    # 2. Parameter Importances\n",
    "    try:\n",
    "        fig = plot_param_importances(study)\n",
    "        fig.update_layout(title='Hyperparameter Importances')\n",
    "        if save_dir:\n",
    "            fig.write_image(os.path.join(save_dir, 'param_importances.png'))\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot parameter importances: {e}\")\n",
    "    \n",
    "    # 3. Parallel Coordinate Plot\n",
    "    try:\n",
    "        fig = plot_parallel_coordinate(study)\n",
    "        fig.update_layout(title='Parallel Coordinate Plot')\n",
    "        if save_dir:\n",
    "            fig.write_image(os.path.join(save_dir, 'parallel_coordinate.png'))\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot parallel coordinate: {e}\")\n",
    "    \n",
    "    # 4. Slice Plot\n",
    "    try:\n",
    "        fig = plot_slice(study)\n",
    "        fig.update_layout(title='Slice Plot')\n",
    "        if save_dir:\n",
    "            fig.write_image(os.path.join(save_dir, 'slice_plot.png'))\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot slice: {e}\")\n",
    "\n",
    "\n",
    "def plot_ranking_metrics(metrics, title=\"BPR-MF Results\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ranking metrics (HR@K, NDCG@K).\n",
    "    \"\"\"\n",
    "    k_values = config.RANKING_K_VALUES\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # HR@K\n",
    "    hr_values = [metrics[f'hr@{k}'] for k in k_values]\n",
    "    bars1 = axes[0].bar([f'HR@{k}' for k in k_values], hr_values, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_ylabel('Hit Rate', fontsize=12)\n",
    "    axes[0].set_title('Hit Rate @ K', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    for bar, v in zip(bars1, hr_values):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "    \n",
    "    # NDCG@K\n",
    "    ndcg_values = [metrics[f'ndcg@{k}'] for k in k_values]\n",
    "    bars2 = axes[1].bar([f'NDCG@{k}' for k in k_values], ndcg_values, color='coral', edgecolor='black')\n",
    "    axes[1].set_ylabel('NDCG', fontsize=12)\n",
    "    axes[1].set_title('NDCG @ K', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    for bar, v in zip(bars2, ndcg_values):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\u2713 Figure saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_section",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, filename='bpr_mf_results.json'):\n",
    "    \"\"\"\n",
    "    Save results to JSON file.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(config.RESULTS_DIR, filename)\n",
    "\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(v) for v in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    serializable_results = convert_to_serializable(results)\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "    print(f\"\\u2713 Results saved to {filepath}\")\n",
    "\n",
    "\n",
    "def print_comparison_table(centralized_metrics):\n",
    "    \"\"\"\n",
    "    Print comparison table between centralized and federated results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'Centralized BPR-MF (Tuned)': {\n",
    "            'HR@10': centralized_metrics.get('hr@10', 0) * 100,\n",
    "            'NDCG@10': centralized_metrics.get('ndcg@10', 0) * 100,\n",
    "            'MRR': centralized_metrics.get('mrr', 0) * 100,\n",
    "            'Privacy': 'No'\n",
    "        },\n",
    "        'Federated BPR-MF (\\u03b1)': {\n",
    "            'HR@10': 59.73,\n",
    "            'NDCG@10': 35.04,\n",
    "            'MRR': 29.18,\n",
    "            'Privacy': 'Yes'\n",
    "        },\n",
    "        'Federated Dual (Concat)': {\n",
    "            'HR@10': 68.32,\n",
    "            'NDCG@10': 42.70,\n",
    "            'MRR': 36.38,\n",
    "            'Privacy': 'Yes'\n",
    "        },\n",
    "        'PFedRec (SOTA)': {\n",
    "            'HR@10': 70.0,\n",
    "            'NDCG@10': 44.36,\n",
    "            'MRR': None,\n",
    "            'Privacy': 'Yes'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON: CENTRALIZED vs FEDERATED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Method':<30} {'HR@10':>10} {'NDCG@10':>10} {'MRR':>10} {'Privacy':>10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for method, metrics in results.items():\n",
    "        hr = f\"{metrics['HR@10']:.2f}%\" if metrics['HR@10'] else 'N/A'\n",
    "        ndcg = f\"{metrics['NDCG@10']:.2f}%\" if metrics['NDCG@10'] else 'N/A'\n",
    "        mrr = f\"{metrics['MRR']:.2f}%\" if metrics['MRR'] else 'N/A'\n",
    "        print(f\"{method:<30} {hr:>10} {ndcg:>10} {mrr:>10} {metrics['Privacy']:>10}\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_section",
   "metadata": {},
   "source": [
    "# Main Execution\n",
    "\n",
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MOVIELENS-1M DATASET\n",
      "============================================================\n",
      "✓ Dataset loaded successfully\n",
      "  - Users: 6,040\n",
      "  - Items: 3,706\n",
      "  - Ratings: 1,000,209\n",
      "  - Sparsity: 95.53%\n",
      "\n",
      "Splitting data (leave-one-out)...\n",
      "✓ Train size: 994,169 ratings\n",
      "✓ Test size:  6,040 ratings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df, num_users, num_items = load_movielens_1m()\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df, train_user_items = train_test_split_leave_one_out(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_section",
   "metadata": {},
   "source": [
    "## Step 2: Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_optuna",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:22:25,528] A new study created in memory with name: bpr_mf_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BAYESIAN OPTIMIZATION (Optuna)\n",
      "============================================================\n",
      "\n",
      "Search Space:\n",
      "  - embedding_dim: [32, 64, 128, 256]\n",
      "  - lr: [0.0001, 0.1] (log)\n",
      "  - weight_decay: [1e-07, 0.0001] (log)\n",
      "  - num_negatives: [1, 2, 4, 8]\n",
      "  - batch_size: [256, 512, 1024, 2048]\n",
      "\n",
      "Optimization Settings:\n",
      "  - Number of trials: 50\n",
      "  - Epochs per trial: 50\n",
      "  - Objective metric: ndcg@10\n",
      "  - Pruning warmup: 10 epochs\n",
      "  - Eval sample users: 1000\n",
      "\n",
      "Starting optimization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.41749:   2%|▏         | 1/50 [07:24<6:02:47, 444.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:29:49,763] Trial 0 finished with value: 0.4174897196745984 and parameters: {'embedding_dim': 64, 'lr': 0.00029380279387035364, 'weight_decay': 2.937538457632828e-07, 'num_negatives': 2, 'batch_size': 512}. Best is trial 0 with value: 0.4174897196745984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.41749:   4%|▍         | 2/50 [11:09<4:12:25, 315.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:33:35,189] Trial 1 finished with value: 0.3793433957464953 and parameters: {'embedding_dim': 256, 'lr': 0.0019762189340280074, 'weight_decay': 7.476312062252299e-07, 'num_negatives': 1, 'batch_size': 512}. Best is trial 0 with value: 0.4174897196745984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.41749:   6%|▌         | 3/50 [16:01<3:58:47, 304.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:38:27,293] Trial 2 finished with value: 0.2589272831566388 and parameters: {'embedding_dim': 128, 'lr': 0.00015673095467235422, 'weight_decay': 7.025166339242167e-05, 'num_negatives': 1, 'batch_size': 256}. Best is trial 0 with value: 0.4174897196745984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.41749:   8%|▊         | 4/50 [34:15<7:52:22, 616.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:56:40,685] Trial 3 finished with value: 0.390211340050466 and parameters: {'embedding_dim': 64, 'lr': 0.0008612579192594886, 'weight_decay': 3.6324869566766083e-06, 'num_negatives': 4, 'batch_size': 256}. Best is trial 0 with value: 0.4174897196745984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  10%|█         | 5/50 [36:52<5:38:07, 450.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 12:59:18,427] Trial 4 finished with value: 0.41960229243815944 and parameters: {'embedding_dim': 256, 'lr': 0.001465655388622534, 'weight_decay': 6.516990611177174e-07, 'num_negatives': 1, 'batch_size': 2048}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  12%|█▏        | 6/50 [41:34<4:48:22, 393.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:03:59,857] Trial 5 finished with value: 0.2718819231966579 and parameters: {'embedding_dim': 256, 'lr': 0.013199942261535026, 'weight_decay': 1.5382308040279007e-05, 'num_negatives': 1, 'batch_size': 256}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  14%|█▍        | 7/50 [59:43<7:24:49, 620.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:22:08,787] Trial 6 finished with value: 0.24296453050555814 and parameters: {'embedding_dim': 128, 'lr': 0.04588156549160976, 'weight_decay': 2.6100256506134772e-06, 'num_negatives': 4, 'batch_size': 256}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  16%|█▌        | 8/50 [1:02:34<5:34:19, 477.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:25:00,018] Trial 7 finished with value: 0.41250006465275335 and parameters: {'embedding_dim': 256, 'lr': 0.0008771380343280567, 'weight_decay': 3.355151022721482e-06, 'num_negatives': 1, 'batch_size': 1024}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  18%|█▊        | 9/50 [1:22:11<7:55:50, 696.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:44:37,413] Trial 8 finished with value: 0.33526629547049175 and parameters: {'embedding_dim': 32, 'lr': 0.025764174425233172, 'weight_decay': 3.628358380354914e-07, 'num_negatives': 8, 'batch_size': 2048}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  20%|██        | 10/50 [1:32:02<7:22:26, 663.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:54:27,839] Trial 9 finished with value: 0.4168029679018979 and parameters: {'embedding_dim': 64, 'lr': 0.001787446325623842, 'weight_decay': 4.6379219034580327e-07, 'num_negatives': 4, 'batch_size': 2048}. Best is trial 4 with value: 0.41960229243815944.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  22%|██▏       | 11/50 [1:34:00<5:22:46, 496.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:56:25,591] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.419602:  24%|██▍       | 12/50 [1:35:28<3:55:55, 372.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 13:57:54,314] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  26%|██▌       | 13/50 [1:42:48<4:02:17, 392.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:05:14,161] Trial 12 finished with value: 0.43735022617742414 and parameters: {'embedding_dim': 256, 'lr': 0.00037133853185137054, 'weight_decay': 1.0679463831061146e-06, 'num_negatives': 2, 'batch_size': 512}. Best is trial 12 with value: 0.43735022617742414.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  28%|██▊       | 14/50 [2:03:16<6:27:00, 645.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:25:41,682] Trial 13 finished with value: 0.4265915316871034 and parameters: {'embedding_dim': 256, 'lr': 0.0006183961600746829, 'weight_decay': 1.2836447245978256e-06, 'num_negatives': 8, 'batch_size': 1024}. Best is trial 12 with value: 0.43735022617742414.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  30%|███       | 15/50 [2:23:41<7:58:16, 819.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:46:06,932] Trial 14 finished with value: 0.435163696545538 and parameters: {'embedding_dim': 256, 'lr': 0.00048648249118773593, 'weight_decay': 1.3382450107499807e-06, 'num_negatives': 8, 'batch_size': 1024}. Best is trial 12 with value: 0.43735022617742414.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  32%|███▏      | 16/50 [2:27:51<6:07:30, 648.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:50:17,516] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  34%|███▍      | 17/50 [2:28:52<4:19:31, 471.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:51:18,509] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  36%|███▌      | 18/50 [2:34:39<3:51:29, 434.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:57:04,535] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  38%|███▊      | 19/50 [2:36:08<2:50:44, 330.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 14:58:33,706] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.43735:  40%|████      | 20/50 [2:40:04<2:31:07, 302.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 15:02:30,231] Trial 19 pruned. \n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization\n",
    "study, best_params = run_bayesian_optimization(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    train_user_items=train_user_items,\n",
    "    num_users=num_users,\n",
    "    num_items=num_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_optuna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization results\n",
    "plot_optimization_results(study, save_dir=config.FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_section",
   "metadata": {},
   "source": [
    "## Step 3: Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_final",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(train_df, test_df, train_user_items, num_users, num_items, params):\n",
    "    \"\"\"\n",
    "    Train final model with best hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING FINAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  - {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    print(f\"  - n_epochs: {config.N_EPOCHS_FINAL}\")\n",
    "    print()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = BPRDataset(\n",
    "        train_df, \n",
    "        num_items, \n",
    "        num_negatives=params['num_negatives']\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BPRMF(\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        embedding_dim=params['embedding_dim'],\n",
    "        use_bias=True\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = datetime.now()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, config.N_EPOCHS_FINAL + 1):\n",
    "        loss = train_epoch_verbose(model, train_loader, optimizer, device, epoch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: BPR Loss = {loss:.4f}\")\n",
    "    \n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"\\n\\u2713 Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics = compute_ranking_metrics(\n",
    "        model=model,\n",
    "        test_df=test_df,\n",
    "        train_user_items=train_user_items,\n",
    "        num_items=num_items,\n",
    "        num_negatives=config.EVAL_NUM_NEGATIVES,\n",
    "        k_values=config.RANKING_K_VALUES,\n",
    "        device=device,\n",
    "        sample_users=None,  # Full evaluation\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BPR-MF RESULTS (Centralized - Bayesian Optimized)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for k in config.RANKING_K_VALUES:\n",
    "        print(f\"HR@{k}:   {metrics[f'hr@{k}']:.4f} ({metrics[f'hr@{k}']*100:.2f}%)\")\n",
    "        print(f\"NDCG@{k}: {metrics[f'ndcg@{k}']:.4f} ({metrics[f'ndcg@{k}']*100:.2f}%)\")\n",
    "        print()\n",
    "    print(f\"MRR:     {metrics['mrr']:.4f} ({metrics['mrr']*100:.2f}%)\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return model, metrics, losses, training_time\n",
    "\n",
    "\n",
    "# Train final model\n",
    "final_model, final_metrics, losses, training_time = train_final_model(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    train_user_items=train_user_items,\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    params=best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_section",
   "metadata": {},
   "source": [
    "## Step 4: Save Results and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_and_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'model_name': 'BPR-MF (Centralized - Bayesian Optimized)',\n",
    "    'dataset': config.DATASET,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'optimization': {\n",
    "        'method': 'Bayesian Optimization (Optuna)',\n",
    "        'sampler': 'TPE',\n",
    "        'pruner': 'MedianPruner',\n",
    "        'n_trials': config.OPTUNA_N_TRIALS,\n",
    "        'objective_metric': config.OPTUNA_METRIC,\n",
    "        'best_trial_value': study.best_trial.value,\n",
    "    },\n",
    "    'best_params': best_params,\n",
    "    'training': {\n",
    "        'n_epochs': config.N_EPOCHS_FINAL,\n",
    "        'final_loss': losses[-1],\n",
    "        'training_time': training_time,\n",
    "    },\n",
    "    'eval_protocol': {\n",
    "        'num_negatives': config.EVAL_NUM_NEGATIVES,\n",
    "        'k_values': config.RANKING_K_VALUES\n",
    "    },\n",
    "    'metrics': final_metrics,\n",
    "    'data_info': {\n",
    "        'num_users': num_users,\n",
    "        'num_items': num_items,\n",
    "        'train_size': len(train_df),\n",
    "        'test_size': len(test_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "save_results(results, 'bpr_mf_centralized_results.json')\n",
    "\n",
    "# Plot metrics\n",
    "plot_ranking_metrics(\n",
    "    final_metrics, \n",
    "    title=\"BPR-MF (Centralized - Bayesian Optimized)\",\n",
    "    save_path=os.path.join(config.FIGURES_DIR, 'bpr_mf_ranking_metrics.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_section",
   "metadata": {},
   "source": [
    "## Step 5: Comparison with Federated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print_comparison_table(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults saved to: {config.RESULTS_DIR}\")\n",
    "print(f\"Figures saved to: {config.FIGURES_DIR}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - bpr_mf_centralized_results.json\")\n",
    "print(\"  - bpr_mf_ranking_metrics.png\")\n",
    "print(\"  - optimization_history.png\")\n",
    "print(\"  - param_importances.png\")\n",
    "print(\"  - parallel_coordinate.png\")\n",
    "print(\"  - slice_plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
