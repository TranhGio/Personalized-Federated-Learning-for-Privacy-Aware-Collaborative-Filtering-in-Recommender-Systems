# =====================================================================
# For a full TOML configuration guide, check the Flower docs:
# https://flower.ai/docs/framework/how-to-configure-pyproject-toml.html
# =====================================================================

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "federated-adaptive-personalized-cf"
version = "1.0.0"
description = ""
license = "Apache-2.0"
# Dependencies for your Flower App
dependencies = [
    "flwr[simulation]>=1.22.0",
    "flwr-datasets[vision]>=0.5.0",
    "torch>=2.7.1",
    "torchvision>=0.22.1",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "wandb>=0.19.0",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "DangVinh"

# Point to your ServerApp and ClientApp objects
[tool.flwr.app.components]
serverapp = "federated_adaptive_personalized_cf.server_app:app"
clientapp = "federated_adaptive_personalized_cf.client_app:app"

# Custom config values accessible via `context.run_config`
[tool.flwr.app.config]
# Federated Learning parameters
num-server-rounds = 50
fraction-train = 1.0
local-epochs = 12

# Federated Learning Strategy
strategy = "fedprox"         # Options: "fedavg" (default) or "fedprox"
proximal-mu = 0.01           # Proximal term strength for FedProx (μ in paper)
                            # When 0.0, FedProx behaves identically to FedAvg
                            # Recommended values: 0.001, 0.01, 0.1, 1.0

# Model parameters
model-type = "bpr"  # Options: "basic" (MSE), "bpr" (BPR), or "dual" (Dual-Level Personalized BPR)
embedding-dim = 128  # Latent factor dimensionality (32, 64, 128, 256)
dropout = 0.1       # Dropout rate for regularization

# =============================================================================
# Dual-Level Personalization Configuration (model-type = "dual")
# =============================================================================
# Novel architecture combining:
# - Level 1: Multi-factor adaptive α for embedding interpolation
# - Level 2: Client-specific MLP for non-linear scoring
#
# This enables both:
# - HOW MUCH to personalize (via statistical α)
# - HOW to score interactions (via PersonalMLP)

mlp-hidden-dims = "512,256,128"    # Hidden layer dimensions for PersonalMLP (comma-separated)
                              # Default: embedding_dim, embedding_dim//2
                              # Smaller = fewer local params, faster training
                              # Larger = more expressive but more overfitting risk

fusion-type = "concat"           # How to combine CF and MLP scores:
                              # "add" - Simple addition (default, no extra params)
                              # "gate" - Learnable gate: g*CF + (1-g)*MLP (1 param)
                              # "concat" - Concatenate and project (3 params)

# Training parameters
lr = 0.005          # Learning rate (Adam optimizer)
weight-decay = 1e-5 # L2 regularization strength
num-negatives = 1   # Number of negative samples per positive (for BPR only)

# Data partitioning
alpha = 0.5         # Dirichlet concentration parameter (lower = more non-IID)

# =============================================================================
# Adaptive Personalization Configuration (α)
# =============================================================================
# Alpha controls personalization level: α → 1 = local, α → 0 = global
# Formula: p̃_u = α * p_local + (1 - α) * p_global

# Alpha method selection
alpha-method = "hierarchical_conditional"     # Options: "data_quantity", "multi_factor", or "hierarchical_conditional"
                                  # - data_quantity: Single factor (interaction count only)
                                  # - multi_factor: 4-factor weighted sum (may have factor conflicts)
                                  # - hierarchical_conditional: Addresses factor conflicts via grouping + rules

# Common alpha parameters
alpha-min = 0.1                    # Minimum personalization (even sparse users get some local)
alpha-max = 0.95                   # Maximum personalization (even dense users get some global)
alpha-quantity-threshold = 100     # Sigmoid midpoint: users with 100 interactions get α ≈ 0.5 (matches ML-1M median)
alpha-quantity-temperature = 0.05  # Sigmoid steepness (lower = gentler transition across user activity levels)

# Multi-factor weights (must sum to 1.0, only used when alpha-method = "multi_factor")
# Formula: α = w_q * f_quantity + w_d * f_diversity + w_c * f_coverage + w_s * f_consistency
alpha-weight-quantity = 0.25       # Weight for interaction count factor (reduced to avoid correlation)
alpha-weight-diversity = 0.35      # Weight for genre entropy factor (increased - more independent variance)
alpha-weight-coverage = 0.20       # Weight for unique items factor
alpha-weight-consistency = 0.20    # Weight for rating stability factor (increased - independent of quantity)

# Multi-factor normalization thresholds (only used when alpha-method = "multi_factor")
alpha-max-entropy = 3.0            # Max genre entropy for normalization (~log2(18) for MovieLens genres)
alpha-coverage-threshold = 100     # Items for full coverage credit
alpha-max-rating-std = 1.5         # Max rating std for normalization (typical for 1-5 scale)

# =============================================================================
# Hierarchical Conditional Alpha Configuration (alpha-method = "hierarchical_conditional")
# =============================================================================
# Addresses two critical conflicts in multi-factor approach:
# 1. Quantity-Coverage Redundancy: Both highly correlated (0.8-1.0)
#    Solution: Combine via geometric mean → data_volume = sqrt(f_q * f_c)
# 2. Diversity-Consistency Conflict: Negatively correlated (-0.3 to -0.5)
#    Solution: Combine via harmonic mean → preference_quality = 2*f_d*f_s/(f_d+f_s)

# Hierarchical weights (must sum to 1.0)
alpha-hc-data-volume-weight = 0.55      # Weight for data_volume = sqrt(quantity * coverage)
alpha-hc-preference-weight = 0.45       # Weight for preference_quality = harmonic(diversity, consistency)

# Conditional rule thresholds (domain-aware adjustments for MovieLens user archetypes)
# Rule 1: Sparse users - users with few interactions get penalty
alpha-hc-sparse-threshold = 20          # Interactions below this trigger sparse penalty
alpha-hc-sparse-penalty-max = 0.5       # Maximum penalty (50% reduction at n=0)

# Rule 2: Niche specialists - low diversity but high quantity users get bonus
alpha-hc-niche-diversity-threshold = 0.25   # f_diversity below this threshold
alpha-hc-niche-quantity-threshold = 0.6     # f_quantity above this threshold
alpha-hc-niche-bonus = 0.15                 # Alpha bonus for niche specialists

# Rule 3: Inconsistent raters - high rating variance users get penalty
alpha-hc-inconsistent-threshold = 0.3       # f_consistency below this triggers penalty
alpha-hc-inconsistent-penalty = 0.3         # 30% penalty for unreliable preferences

# Rule 4: Completionists - high coverage but low diversity (genre experts)
alpha-hc-completionist-coverage = 0.7       # f_coverage above this threshold
alpha-hc-completionist-diversity = 0.3      # f_diversity below this threshold
alpha-hc-completionist-bonus = 0.1          # Alpha bonus for completionists

# Global prototype aggregation
prototype-momentum = 0.9           # EMA momentum: p_global = m * p_old + (1-m) * p_new
                                   # Higher = more stable but slower adaptation

# User group boundaries for per-group metrics
# Groups: sparse (0-30), medium (30-100), dense (100+) interactions
user-group-sparse = "0,30"
user-group-medium = "30,100"
user-group-dense = "100,10000"

# Ranking evaluation
enable-ranking-eval = true        # Enable comprehensive ranking metrics evaluation
ranking-k-values = "5,10,20"      # K values for Hit Rate, Precision, Recall, NDCG metrics (comma-separated)

# Sampled evaluation (leave-one-out with N negatives)
# This follows the evaluation protocol used in NCF, FedMF, PFedRec papers
# for fair comparison with published baselines
eval-num-negatives = 99           # Number of negative samples per positive (default: 99 as in NCF paper)

# =============================================================================
# Early Stopping Configuration
# =============================================================================
# Stop training when the monitored metric stops improving
early-stopping-enabled = true     # Enable/disable early stopping (enable for sweeps)
early-stopping-patience = 10       # Rounds without improvement before stopping
early-stopping-metric = "sampled_ndcg@10"  # Metric to monitor (sampled_ndcg@10, ndcg@10, hit_rate@10)
early-stopping-mode = "max"        # "max" for metrics where higher is better, "min" for lower
early-stopping-min-delta = 0.001   # Minimum change to qualify as improvement

# Weights & Biases (wandb) configuration
wandb-enabled = true              # Enable/disable wandb logging
wandb-project = "federated-adaptive-personalized-cf"    # wandb project name
wandb-entity = ""                 # wandb team/entity (optional, uses default if empty)
wandb-run-name = ""               # Custom run name (optional, auto-generated if empty)

# Default federation to use when running the app
[tool.flwr.federations]
default = "local-sim-gpu"

# Local simulation federation (CPU-only)
[tool.flwr.federations.local-simulation]
options.num-supernodes = 5

# GPU-enabled simulation federation (requires PyTorch with CUDA support)
# Run with: flwr run . local-sim-gpu
[tool.flwr.federations.local-sim-gpu]
options.num-supernodes = 5
options.backend.client-resources.num-cpus = 6
options.backend.client-resources.num-gpus = 0.2

# Remote federation example for use with SuperLink
[tool.flwr.federations.remote-federation]
address = "<SUPERLINK-ADDRESS>:<PORT>"
insecure = true  # Remove this line to enable TLS
# root-certificates = "<PATH/TO/ca.crt>"  # For TLS setup
