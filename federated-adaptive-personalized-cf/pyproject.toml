# =====================================================================
# For a full TOML configuration guide, check the Flower docs:
# https://flower.ai/docs/framework/how-to-configure-pyproject-toml.html
# =====================================================================

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "federated-adaptive-personalized-cf"
version = "1.0.0"
description = ""
license = "Apache-2.0"
# Dependencies for your Flower App
dependencies = [
    "flwr[simulation]>=1.22.0",
    "flwr-datasets[vision]>=0.5.0",
    "torch>=2.7.1",
    "torchvision>=0.22.1",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "wandb>=0.19.0",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "DangVinh"

# Point to your ServerApp and ClientApp objects
[tool.flwr.app.components]
serverapp = "federated_adaptive_personalized_cf.server_app:app"
clientapp = "federated_adaptive_personalized_cf.client_app:app"

# Custom config values accessible via `context.run_config`
[tool.flwr.app.config]
# Federated Learning parameters
num-server-rounds = 50
fraction-train = 1.0
local-epochs = 12

# Federated Learning Strategy
strategy = "fedavg"         # Options: "fedavg" (default) or "fedprox"
proximal-mu = 0.01           # Proximal term strength for FedProx (μ in paper)
                            # When 0.0, FedProx behaves identically to FedAvg
                            # Recommended values: 0.001, 0.01, 0.1, 1.0

# Model parameters
model-type = "bpr"  # Options: "basic" (MSE), "bpr" (BPR), or "dual" (Dual-Level Personalized BPR)
embedding-dim = 128  # Latent factor dimensionality (32, 64, 128, 256)
dropout = 0.1       # Dropout rate for regularization

# =============================================================================
# Dual-Level Personalization Configuration (model-type = "dual")
# =============================================================================
# Novel architecture combining:
# - Level 1: Multi-factor adaptive α for embedding interpolation
# - Level 2: Client-specific MLP for non-linear scoring
#
# This enables both:
# - HOW MUCH to personalize (via statistical α)
# - HOW to score interactions (via PersonalMLP)

mlp-hidden-dims = "512,256,128"    # Hidden layer dimensions for PersonalMLP (comma-separated)
                              # Default: embedding_dim, embedding_dim//2
                              # Smaller = fewer local params, faster training
                              # Larger = more expressive but more overfitting risk

fusion-type = "concat"           # How to combine CF and MLP scores:
                              # "add" - Simple addition (default, no extra params)
                              # "gate" - Learnable gate: g*CF + (1-g)*MLP (1 param)
                              # "concat" - Concatenate and project (3 params)

# Training parameters
lr = 0.005          # Learning rate (Adam optimizer)
weight-decay = 1e-5 # L2 regularization strength
num-negatives = 1   # Number of negative samples per positive (for BPR only)

# Data partitioning
alpha = 0.5         # Dirichlet concentration parameter (lower = more non-IID)

# =============================================================================
# Adaptive Personalization Configuration (α)
# =============================================================================
# Alpha controls personalization level: α → 1 = local, α → 0 = global
# Formula: p̃_u = α * p_local + (1 - α) * p_global

# Alpha method selection
alpha-method = "multi_factor"     # Options: "data_quantity" (single factor) or "multi_factor" (4 factors)

# Common alpha parameters
alpha-min = 0.1                    # Minimum personalization (even sparse users get some local)
alpha-max = 0.95                   # Maximum personalization (even dense users get some global)
alpha-quantity-threshold = 100     # Sigmoid midpoint: users with 100 interactions get α ≈ 0.5 (matches ML-1M median)
alpha-quantity-temperature = 0.05  # Sigmoid steepness (lower = gentler transition across user activity levels)

# Multi-factor weights (must sum to 1.0, only used when alpha-method = "multi_factor")
# Formula: α = w_q * f_quantity + w_d * f_diversity + w_c * f_coverage + w_s * f_consistency
alpha-weight-quantity = 0.25       # Weight for interaction count factor (reduced to avoid correlation)
alpha-weight-diversity = 0.35      # Weight for genre entropy factor (increased - more independent variance)
alpha-weight-coverage = 0.20       # Weight for unique items factor
alpha-weight-consistency = 0.20    # Weight for rating stability factor (increased - independent of quantity)

# Multi-factor normalization thresholds (only used when alpha-method = "multi_factor")
alpha-max-entropy = 3.0            # Max genre entropy for normalization (~log2(18) for MovieLens genres)
alpha-coverage-threshold = 100     # Items for full coverage credit
alpha-max-rating-std = 1.5         # Max rating std for normalization (typical for 1-5 scale)

# Global prototype aggregation
prototype-momentum = 0.9           # EMA momentum: p_global = m * p_old + (1-m) * p_new
                                   # Higher = more stable but slower adaptation

# User group boundaries for per-group metrics
# Groups: sparse (0-30), medium (30-100), dense (100+) interactions
user-group-sparse = "0,30"
user-group-medium = "30,100"
user-group-dense = "100,10000"

# Ranking evaluation
enable-ranking-eval = true        # Enable comprehensive ranking metrics evaluation
ranking-k-values = "5,10,20"      # K values for Hit Rate, Precision, Recall, NDCG metrics (comma-separated)

# Sampled evaluation (leave-one-out with N negatives)
# This follows the evaluation protocol used in NCF, FedMF, PFedRec papers
# for fair comparison with published baselines
eval-num-negatives = 99           # Number of negative samples per positive (default: 99 as in NCF paper)

# Weights & Biases (wandb) configuration
wandb-enabled = true              # Enable/disable wandb logging
wandb-project = "federated-adaptive-personalized-cf"    # wandb project name
wandb-entity = ""                 # wandb team/entity (optional, uses default if empty)
wandb-run-name = ""               # Custom run name (optional, auto-generated if empty)

# Default federation to use when running the app
[tool.flwr.federations]
default = "local-sim-gpu"

# Local simulation federation (CPU-only)
[tool.flwr.federations.local-simulation]
options.num-supernodes = 5

# GPU-enabled simulation federation (requires PyTorch with CUDA support)
# Run with: flwr run . local-sim-gpu
[tool.flwr.federations.local-sim-gpu]
options.num-supernodes = 5
options.backend.client-resources.num-cpus = 6
options.backend.client-resources.num-gpus = 0.2

# Remote federation example for use with SuperLink
[tool.flwr.federations.remote-federation]
address = "<SUPERLINK-ADDRESS>:<PORT>"
# WARNING: insecure = true disables TLS/certificate verification and sends all federated
# training traffic (including model updates) unencrypted over the network. This is a
# SECURITY RISK in production environments. Use only for local development/testing.
# For production, remove the insecure flag and configure proper certificates instead.
insecure = true  # Remove this line to enable TLS
# root-certificates = "<PATH/TO/ca.crt>"  # For TLS setup
