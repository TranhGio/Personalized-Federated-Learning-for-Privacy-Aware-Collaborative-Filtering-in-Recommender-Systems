# =====================================================================
# For a full TOML configuration guide, check the Flower docs:
# https://flower.ai/docs/framework/how-to-configure-pyproject-toml.html
# =====================================================================

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "federated-adaptive-personalized-cf"
version = "1.0.0"
description = ""
license = "Apache-2.0"
# Dependencies for your Flower App
dependencies = [
    "flwr[simulation]>=1.22.0",
    "flwr-datasets[vision]>=0.5.0",
    "torch>=2.7.1",
    "torchvision>=0.22.1",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "wandb=0.23.1",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "DangVinh"

# Point to your ServerApp and ClientApp objects
[tool.flwr.app.components]
serverapp = "federated_adaptive_personalized_cf.server_app:app"
clientapp = "federated_adaptive_personalized_cf.client_app:app"

# Custom config values accessible via `context.run_config`
[tool.flwr.app.config]
# Federated Learning parameters
num-server-rounds = 10
fraction-train = 1.0
local-epochs = 5

# Federated Learning Strategy
strategy = "fedavg"         # Options: "fedavg" (default) or "fedprox"
proximal-mu = 0.01           # Proximal term strength for FedProx (μ in paper)
                            # When 0.0, FedProx behaves identically to FedAvg
                            # Recommended values: 0.001, 0.01, 0.1, 1.0

# Model parameters
model-type = "bpr"  # Options: "basic" (MSE) or "bpr" (Bayesian Personalized Ranking)
embedding-dim = 128  # Latent factor dimensionality (32, 64, 128, 256)
dropout = 0.1       # Dropout rate for regularization

# Training parameters
lr = 0.005          # Learning rate (Adam optimizer)
weight-decay = 1e-5 # L2 regularization strength
num-negatives = 1   # Number of negative samples per positive (for BPR only)

# Data partitioning
alpha = 0.5         # Dirichlet concentration parameter (lower = more non-IID)

# =============================================================================
# Adaptive Personalization Configuration (α)
# =============================================================================
# Alpha controls personalization level: α → 1 = local, α → 0 = global
# Formula: p̃_u = α * p_local + (1 - α) * p_global

# Alpha computation parameters (DataQuantity method)
alpha-min = 0.1                    # Minimum personalization (even sparse users get some local)
alpha-max = 0.95                   # Maximum personalization (even dense users get some global)
alpha-quantity-threshold = 50      # Sigmoid midpoint: users with 50 interactions get α ≈ 0.5
alpha-quantity-temperature = 0.1   # Sigmoid steepness (higher = sharper transition)

# Global prototype aggregation
prototype-momentum = 0.9           # EMA momentum: p_global = m * p_old + (1-m) * p_new
                                   # Higher = more stable but slower adaptation

# User group boundaries for per-group metrics
# Groups: sparse (0-30), medium (30-100), dense (100+) interactions
user-group-sparse = "0,30"
user-group-medium = "30,100"
user-group-dense = "100,10000"

# Ranking evaluation
enable-ranking-eval = true        # Enable comprehensive ranking metrics evaluation
ranking-k-values = "5,10,20"      # K values for Hit Rate, Precision, Recall, NDCG metrics (comma-separated)

# Weights & Biases (wandb) configuration
wandb-enabled = true              # Enable/disable wandb logging
wandb-project = "federated-adaptive-personalized-cf"    # wandb project name
wandb-entity = ""                 # wandb team/entity (optional, uses default if empty)
wandb-run-name = ""               # Custom run name (optional, auto-generated if empty)

# Default federation to use when running the app
[tool.flwr.federations]
default = "local-simulation"

# Local simulation federation (CPU-only)
[tool.flwr.federations.local-simulation]
options.num-supernodes = 5

# GPU-enabled simulation federation (requires PyTorch with CUDA support)
# Run with: flwr run . local-sim-gpu
[tool.flwr.federations.local-sim-gpu]
options.num-supernodes = 5
options.backend.client-resources.num-cpus = 12
options.backend.client-resources.num-gpus = 0.2

# Remote federation example for use with SuperLink
[tool.flwr.federations.remote-federation]
address = "<SUPERLINK-ADDRESS>:<PORT>"
insecure = true  # Remove this line to enable TLS
# root-certificates = "<PATH/TO/ca.crt>"  # For TLS setup
